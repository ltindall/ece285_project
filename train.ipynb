{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainging for monet2photo at epoch [0/200]\n",
      "\n",
      "Generated images will be saved in ./generated_images/monet2photo\n",
      "Model checkpoints will be saved in ./checkpoints/monet2photo \n",
      "\n",
      "GPU is enabled \n",
      "\n",
      "[D loss: 1.6980024576187134] [G loss: 10.797660827636719, adverserial: 2.2861692905426025, cycle: 0.5737398862838745, identity: 0.5548185706138611]\n",
      "[D loss: 3.2969393730163574] [G loss: 9.682466506958008, adverserial: 2.1321258544921875, cycle: 0.5139608383178711, identity: 0.48214638233184814]\n",
      "[D loss: 6.20265007019043] [G loss: 13.379976272583008, adverserial: 6.513184547424316, cycle: 0.4705236852169037, identity: 0.43231087923049927]\n",
      "[D loss: 2.0780158042907715] [G loss: 9.674233436584473, adverserial: 3.1141414642333984, cycle: 0.4482785165309906, identity: 0.41546139121055603]\n",
      "[D loss: 1.7755733728408813] [G loss: 9.097246170043945, adverserial: 1.6674842834472656, cycle: 0.4944833517074585, identity: 0.49698561429977417]\n",
      "[D loss: 2.36171817779541] [G loss: 10.2944974899292, adverserial: 3.3806912899017334, cycle: 0.4576227068901062, identity: 0.4675157070159912]\n",
      "[D loss: 1.9072539806365967] [G loss: 9.561551094055176, adverserial: 3.0364387035369873, cycle: 0.43092572689056396, identity: 0.4431709051132202]\n",
      "[D loss: 1.164747714996338] [G loss: 8.069777488708496, adverserial: 1.8202569484710693, cycle: 0.4143248200416565, identity: 0.4212545156478882]\n",
      "[D loss: 1.4115043878555298] [G loss: 8.330621719360352, adverserial: 1.758866310119629, cycle: 0.4357140362262726, identity: 0.44292300939559937]\n",
      "[D loss: 1.5530157089233398] [G loss: 8.180976867675781, adverserial: 2.415956735610962, cycle: 0.37623822689056396, identity: 0.4005276560783386]\n",
      "[D loss: 1.2602648735046387] [G loss: 7.193770408630371, adverserial: 2.0150978565216064, cycle: 0.33670181035995483, identity: 0.3623307943344116]\n",
      "[D loss: 0.8528152108192444] [G loss: 6.060483932495117, adverserial: 1.2666245698928833, cycle: 0.31118306517601013, identity: 0.3364056944847107]\n",
      "[D loss: 1.1279126405715942] [G loss: 8.14014720916748, adverserial: 1.4861780405044556, cycle: 0.43573471903800964, identity: 0.459324449300766]\n",
      "[D loss: 1.44173002243042] [G loss: 8.281574249267578, adverserial: 2.2384390830993652, cycle: 0.39523428678512573, identity: 0.41815832257270813]\n",
      "[D loss: 1.5286803245544434] [G loss: 7.784119606018066, adverserial: 2.3269219398498535, cycle: 0.35920029878616333, identity: 0.37303900718688965]\n",
      "[D loss: 1.4474962949752808] [G loss: 7.267642974853516, adverserial: 2.057596206665039, cycle: 0.34617409110069275, identity: 0.3496611416339874]\n",
      "[D loss: 1.2579548358917236] [G loss: 9.37291145324707, adverserial: 1.267383337020874, cycle: 0.5261238217353821, identity: 0.5688579678535461]\n",
      "[D loss: 1.243175745010376] [G loss: 8.281907081604004, adverserial: 1.2593239545822144, cycle: 0.4505371153354645, identity: 0.5034424066543579]\n",
      "[D loss: 1.9878307580947876] [G loss: 8.638195037841797, adverserial: 2.845207691192627, cycle: 0.37401771545410156, identity: 0.4105621576309204]\n",
      "[D loss: 1.7213400602340698] [G loss: 7.407771587371826, adverserial: 2.5857341289520264, cycle: 0.3136773705482483, identity: 0.3370527923107147]\n",
      "[D loss: 1.122107982635498] [G loss: 7.755642890930176, adverserial: 1.475473403930664, cycle: 0.41818955540657043, identity: 0.4196547865867615]\n",
      "[D loss: 0.9603670835494995] [G loss: 7.1355390548706055, adverserial: 1.260664939880371, cycle: 0.3894660472869873, identity: 0.39604276418685913]\n",
      "[D loss: 0.660405158996582] [G loss: 6.008045196533203, adverserial: 0.8644697666168213, cycle: 0.340963751077652, identity: 0.34678763151168823]\n",
      "[D loss: 0.49843287467956543] [G loss: 5.037525653839111, adverserial: 0.5649199485778809, cycle: 0.2963789701461792, identity: 0.3017631769180298]\n",
      "[D loss: 0.7234706878662109] [G loss: 6.868744373321533, adverserial: 0.7726585865020752, cycle: 0.4066047668457031, identity: 0.40600764751434326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[D loss: 0.7197531461715698] [G loss: 5.699855804443359, adverserial: 0.6298036575317383, cycle: 0.3389732837677002, identity: 0.33606380224227905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fba40b3aeb8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 347, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 568) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d2885a3428bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Backpropagate through the generator networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "from src_code.models import *\n",
    "from src_code.data_loader import *\n",
    "\n",
    "######################################\n",
    "# Hyperparameters\n",
    "######################################\n",
    "\n",
    "# control verbose output\n",
    "verbose = True\n",
    "\n",
    "# epoch to start training from\n",
    "start_epoch = 0\n",
    "\n",
    "# number of training epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# dataset name\n",
    "dataset_name = \"monet2photo\"\n",
    "\n",
    "# batch size\n",
    "batch_size = 1\n",
    "\n",
    "# validation batch size\n",
    "val_batch_size = 5\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0002\n",
    "\n",
    "# starting epoch for weight decay\n",
    "decay_epoch = 100\n",
    "\n",
    "# training image height\n",
    "img_height= 256\n",
    "\n",
    "# training image width\n",
    "img_width = 256\n",
    "\n",
    "# create sample every n batches\n",
    "sample_val_batch = 100\n",
    "\n",
    "# create model checkpoints every n epochs\n",
    "checkpoint_epoch = 1\n",
    "\n",
    "# patch size for Patch Discriminator\n",
    "patch_size = (1, img_height // 2**4, img_width // 2**4)\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting trainging for %s at epoch [%d/%d]\\n\" % (dataset_name, start_epoch, n_epochs))\n",
    "\n",
    "\n",
    "# Make directory for generated images and model checkpoints\n",
    "os.makedirs('generated_images/%s' % dataset_name, exist_ok=True)\n",
    "os.makedirs('checkpoints/%s' % dataset_name, exist_ok=True)\n",
    "print(\"Generated images will be saved in ./generated_images/%s\" % dataset_name)\n",
    "print(\"Model checkpoints will be saved in ./checkpoints/%s \\n\" % dataset_name)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Load datasets\n",
    "#############################################\n",
    "# Training set\n",
    "train_dataloader = DataLoader(CycleGAN_Dataset(\"datasets/%s/trainA/\" % dataset_name,\"datasets/%s/trainB/\" % dataset_name, img_height, img_width),batch_size=batch_size, num_workers=4)\n",
    "# Validation set\n",
    "val_dataloader = DataLoader(CycleGAN_Dataset(\"datasets/%s/testA/\" % dataset_name, \"datasets/%s/testB/\" % dataset_name, img_height, img_width),batch_size=val_batch_size, num_workers=1)\n",
    "#############################################\n",
    "\n",
    "#############################################\n",
    "# Loss functions and loss weights\n",
    "#############################################\n",
    "# Losses\n",
    "loss_GAN = torch.nn.MSELoss()\n",
    "loss_cycle = torch.nn.L1Loss()\n",
    "loss_identity = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weights\n",
    "lambda_cyc = 10\n",
    "lambda_id = 0.5 * lambda_cyc\n",
    "#############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Create Networks and initialize weights\n",
    "###############################################\n",
    "# Generator and Discriminator Networks\n",
    "G_AB = ResNet_Generator(res_blocks=9)\n",
    "G_BA = ResNet_Generator(res_blocks=9)\n",
    "D_A = Patch_Discriminator()\n",
    "D_B = Patch_Discriminator()\n",
    "\n",
    "# Prep GPU\n",
    "GPU = torch.cuda.is_available()\n",
    "print(\"GPU is {}enabled \\n\".format(['not ', ''][GPU]))\n",
    "\n",
    "if GPU:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "\n",
    "if start_epoch != 0:\n",
    "    G_AB.load_state_dict(torch.load('checkpoints/%s/G_AB_%d.pth' % (dataset_name, start_epoch)))\n",
    "    G_BA.load_state_dict(torch.load('checkpoints/%s/G_BA_%d.pth' % (dataset_name, start_epoch)))\n",
    "    D_A.load_state_dict(torch.load('checkpoints/%s/D_A_%d.pth' % (dataset_name, start_epoch)))\n",
    "    D_B.load_state_dict(torch.load('checkpoints/%s/D_B_%d.pth' % (dataset_name, start_epoch)))\n",
    "else:\n",
    "    G_AB.apply(weights_init)\n",
    "    G_BA.apply(weights_init)\n",
    "    D_A.apply(weights_init)\n",
    "    D_B.apply(weights_init)\n",
    "###############################################\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Pytorch optimizers and learning rate schedulers\n",
    "#################################################\n",
    "# Optimizers\n",
    "G_params = itertools.chain(G_AB.parameters(), G_BA.parameters())\n",
    "optimizer_G = torch.optim.Adam(G_params,lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# Learning rate schedulers\n",
    "# begin to anneal the learning rate after decay_epoch epochs have passed\n",
    "lambdaGAN = lambda epoch: 1.0 - max(0, epoch + start_epoch - decay_epoch) / (n_epochs - decay_epoch)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambdaGAN)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambdaGAN)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambdaGAN)\n",
    "#################################################\n",
    "\n",
    "\n",
    "###################################\n",
    "# Replay buffers\n",
    "###################################\n",
    "buffer_size=50\n",
    "fake_A_buffer = []\n",
    "fake_B_buffer = []\n",
    "\n",
    "def get_buffer_batch(fake_buffer, fakes, buffer_size=50):\n",
    "\n",
    "    mini_batch = []\n",
    "    for f in fakes:\n",
    "        if len(fake_buffer) < buffer_size:\n",
    "            fake_buffer.append(f)\n",
    "            mini_batch.append(f)\n",
    "        else:\n",
    "            if np.random.randint(0,2,1):\n",
    "                i = np.random.randint(buffer_size)\n",
    "                mini_batch.append(fake_buffer[i].clone())\n",
    "                fake_buffer[i] = f\n",
    "            else:\n",
    "                mini_batch.append(f)\n",
    "\n",
    "    return Variable(torch.cat(mini_batch)).view(-1,3, 256, 256)\n",
    "###############################################################\n",
    "\n",
    "#####################################\n",
    "# Generate Validation images\n",
    "#####################################\n",
    "def generate_val_imgs(epoch_count, batch_count):\n",
    "    val_imgs = next(iter(val_dataloader))\n",
    "\n",
    "    val_real_A = Variable(torch.FloatTensor(val_imgs['A']))\n",
    "    val_real_B = Variable(torch.FloatTensor(val_imgs['B']))\n",
    "    if GPU:\n",
    "        val_real_A = val_real_A.cuda()\n",
    "        val_real_B = val_real_B.cuda()\n",
    "\n",
    "    val_fake_A = G_BA(val_real_B.detach())\n",
    "    val_fake_B = G_AB(val_real_A.detach())\n",
    "\n",
    "    stacked_image = torch.cat((val_real_A.data, val_fake_B.data, val_real_B.data, val_fake_A.data), 0)\n",
    "    save_image(stacked_image, 'generated_images/%s/%d_%d.png' % (dataset_name, epoch_count, batch_count), nrow=val_batch_size, normalize=True)\n",
    "#####################################\n",
    "\n",
    "\n",
    "################################################\n",
    "# Training loop\n",
    "################################################\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    for i, AB_img_batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get batch of images from A and B set\n",
    "        real_A = Variable(torch.FloatTensor(AB_img_batch['A']))\n",
    "        real_B = Variable(torch.FloatTensor(AB_img_batch['B']))\n",
    "\n",
    "        # Create matrix of real and fake target values for discriminator MSE loss\n",
    "        real = Variable(torch.FloatTensor(np.ones((real_A.size(0), *patch_size))), requires_grad=False)\n",
    "        fake = Variable(torch.FloatTensor(np.zeros((real_A.size(0), *patch_size))), requires_grad=False)\n",
    "\n",
    "        # Send variables to gpu\n",
    "        if GPU:\n",
    "            real_A = real_A.cuda()\n",
    "            real_B = real_B.cuda()\n",
    "            real = real.cuda()\n",
    "            fake = fake.cuda()\n",
    "\n",
    "        ##############################################\n",
    "        # Train the Generator Networks simultaneously\n",
    "        ##############################################\n",
    "\n",
    "        # zero out G Net gradients\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Adverserial (GAN) loss\n",
    "        ## Generate AB fakes, send through D nets and compute MSE loss\n",
    "        fake_A = G_BA(real_B)\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_GAN_AB = loss_GAN(D_B(fake_B), real)\n",
    "        loss_GAN_BA = loss_GAN(D_A(fake_A), real)\n",
    "        total_loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        ## Send generated fakes back through opposite G nets, compute L1 loss with original reals\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_A = loss_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = loss_cycle(recov_B, real_B)\n",
    "        total_loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Identity loss\n",
    "        ## Send real imgs through their own generator, compute L1 loss with original reals\n",
    "        loss_id_A = loss_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = loss_identity(G_AB(real_B), real_B)\n",
    "        total_loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # Total GAN loss (weighted sum of adverserial, cycle, and identity losses)\n",
    "        loss_G = total_loss_GAN + (lambda_cyc * total_loss_cycle) + (lambda_id * total_loss_identity)\n",
    "\n",
    "        # Backpropagate through the generator networks\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "        # Train the A Discriminator Network\n",
    "        ####################################\n",
    "\n",
    "        # zero out D_A gradients\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # compute loss using real and fake A imgs sent through D_A\n",
    "        loss_real = loss_GAN(D_A(real_A), real)\n",
    "        fake_A_ = get_buffer_batch(fake_A_buffer, fake_A, buffer_size=50)\n",
    "        loss_fake = loss_GAN(D_A(fake_A_.detach()), fake)\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # backpropagate D_A network\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        ####################################\n",
    "\n",
    "\n",
    "        ####################################\n",
    "        # Train the B Discriminator Network\n",
    "        ####################################\n",
    "\n",
    "        # zero out D_B gradients\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # compute loss using real and fake B imgs sent through D_B\n",
    "        loss_real = loss_GAN(D_B(real_B), real)\n",
    "        fake_B_ = get_buffer_batch(fake_B_buffer, fake_B, buffer_size=50)\n",
    "        loss_fake = loss_GAN(D_B(fake_B_.detach()), fake)\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # backpropagate D_B network\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        ##############################################\n",
    "\n",
    "\n",
    "        # Total Discriminator loss from both networks\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # Print the loss values every batch if verbose is enabled\n",
    "        if verbose:\n",
    "            description = \"\\r[D loss: {}] [G loss: {}, adverserial: {}, cycle: {}, identity: {}]\".format(loss_D.item(), loss_G.item(), total_loss_GAN.item(), total_loss_cycle.item(), total_loss_identity.item())\n",
    "            print(description)\n",
    "\n",
    "        # Periodicaly generate images from validation set\n",
    "        if i % sample_val_batch == 0:\n",
    "            generate_val_imgs(epoch, i)\n",
    "\n",
    "\n",
    "    # Update the learning rates using annealing scheduler\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # save model files every epoch\n",
    "    if epoch % checkpoint_epoch == 0:\n",
    "        torch.save(G_AB.state_dict(), 'checkpoints/%s/G_AB_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(G_BA.state_dict(), 'checkpoints/%s/G_BA_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), 'checkpoints/%s/D_A_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), 'checkpoints/%s/D_B_%d.pth' % (dataset_name, epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
