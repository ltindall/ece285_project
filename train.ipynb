{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "from src_code.models import *\n",
    "from src_code.data_loader import *\n",
    "\n",
    "######################################\n",
    "# Hyperparameters\n",
    "######################################\n",
    "\n",
    "# control verbose output\n",
    "verbose = True\n",
    "\n",
    "# epoch to start training from\n",
    "start_epoch = 0\n",
    "\n",
    "# number of training epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# dataset name\n",
    "dataset_name = \"monet2photo\"\n",
    "\n",
    "# batch size\n",
    "batch_size = 1\n",
    "\n",
    "# validation batch size\n",
    "val_batch_size = 5\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0002\n",
    "\n",
    "# starting epoch for weight decay\n",
    "decay_epoch = 100\n",
    "\n",
    "# training image height\n",
    "img_height= 256\n",
    "\n",
    "# training image width\n",
    "img_width = 256\n",
    "\n",
    "# create sample every n batches\n",
    "sample_val_batch = 100\n",
    "\n",
    "# create model checkpoints every n epochs\n",
    "checkpoint_epoch = 1\n",
    "\n",
    "# patch size for Patch Discriminator\n",
    "patch_size = (1, img_height // 2**4, img_width // 2**4)\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting trainging for %s at epoch [%d/%d]\\n\" % (dataset_name, start_epoch, n_epochs))\n",
    "\n",
    "\n",
    "# Make directory for generated images and model checkpoints\n",
    "os.makedirs('generated_images/%s' % dataset_name, exist_ok=True)\n",
    "os.makedirs('checkpoints/%s' % dataset_name, exist_ok=True)\n",
    "print(\"Generated images will be saved in ./generated_images/%s\" % dataset_name)\n",
    "print(\"Model checkpoints will be saved in ./checkpoints/%s \\n\" % dataset_name)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Load datasets\n",
    "#############################################\n",
    "# Training set\n",
    "train_dataloader = DataLoader(CycleGAN_Dataset(\"datasets/%s/trainA/\" % dataset_name,\"datasets/%s/trainB/\" % dataset_name, img_height, img_width),batch_size=batch_size, num_workers=4)\n",
    "# Validation set\n",
    "val_dataloader = DataLoader(CycleGAN_Dataset(\"datasets/%s/testA/\" % dataset_name, \"datasets/%s/testB/\" % dataset_name, img_height, img_width),batch_size=val_batch_size, num_workers=1)\n",
    "#############################################\n",
    "\n",
    "#############################################\n",
    "# Loss functions and loss weights\n",
    "#############################################\n",
    "# Losses\n",
    "loss_GAN = torch.nn.MSELoss()\n",
    "loss_cycle = torch.nn.L1Loss()\n",
    "loss_identity = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weights\n",
    "lambda_cyc = 10\n",
    "lambda_id = 0.5 * lambda_cyc\n",
    "#############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Create Networks and initialize weights\n",
    "###############################################\n",
    "# Generator and Discriminator Networks\n",
    "G_AB = ResNet_Generator(res_blocks=9)\n",
    "G_BA = ResNet_Generator(res_blocks=9)\n",
    "D_A = Patch_Discriminator()\n",
    "D_B = Patch_Discriminator()\n",
    "\n",
    "# Prep GPU\n",
    "GPU = torch.cuda.is_available()\n",
    "print(\"GPU is {}enabled \\n\".format(['not ', ''][GPU]))\n",
    "\n",
    "if GPU:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "\n",
    "if start_epoch != 0:\n",
    "    G_AB.load_state_dict(torch.load('checkpoints/%s/G_AB_%d.pth' % (dataset_name, start_epoch)))\n",
    "    G_BA.load_state_dict(torch.load('checkpoints/%s/G_BA_%d.pth' % (dataset_name, start_epoch)))\n",
    "    D_A.load_state_dict(torch.load('checkpoints/%s/D_A_%d.pth' % (dataset_name, start_epoch)))\n",
    "    D_B.load_state_dict(torch.load('checkpoints/%s/D_B_%d.pth' % (dataset_name, start_epoch)))\n",
    "else:\n",
    "    G_AB.apply(weights_init)\n",
    "    G_BA.apply(weights_init)\n",
    "    D_A.apply(weights_init)\n",
    "    D_B.apply(weights_init)\n",
    "###############################################\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Pytorch optimizers and learning rate schedulers\n",
    "#################################################\n",
    "# Optimizers\n",
    "G_params = itertools.chain(G_AB.parameters(), G_BA.parameters())\n",
    "optimizer_G = torch.optim.Adam(G_params,lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# Learning rate schedulers\n",
    "# begin to anneal the learning rate after decay_epoch epochs have passed\n",
    "lambdaGAN = lambda epoch: 1.0 - max(0, epoch + start_epoch - decay_epoch) / (n_epochs - decay_epoch)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambdaGAN)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambdaGAN)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambdaGAN)\n",
    "#################################################\n",
    "\n",
    "\n",
    "###################################\n",
    "# Replay buffers\n",
    "###################################\n",
    "buffer_size=50\n",
    "fake_A_buffer = []\n",
    "fake_B_buffer = []\n",
    "\n",
    "def get_buffer_batch(fake_buffer, fakes, buffer_size=50):\n",
    "\n",
    "    mini_batch = []\n",
    "    for f in fakes:\n",
    "        if len(fake_buffer) < buffer_size:\n",
    "            fake_buffer.append(f)\n",
    "            mini_batch.append(f)\n",
    "        else:\n",
    "            if np.random.randint(0,2,1):\n",
    "                i = np.random.randint(buffer_size)\n",
    "                mini_batch.append(fake_buffer[i].clone())\n",
    "                fake_buffer[i] = f\n",
    "            else:\n",
    "                mini_batch.append(f)\n",
    "\n",
    "    return Variable(torch.cat(mini_batch)).view(-1,3, 256, 256)\n",
    "###############################################################\n",
    "\n",
    "#####################################\n",
    "# Generate Validation images\n",
    "#####################################\n",
    "def generate_val_imgs(epoch_count, batch_count):\n",
    "    val_imgs = next(iter(val_dataloader))\n",
    "\n",
    "    val_real_A = Variable(torch.FloatTensor(val_imgs['A']))\n",
    "    val_real_B = Variable(torch.FloatTensor(val_imgs['B']))\n",
    "    if GPU:\n",
    "        val_real_A = val_real_A.cuda()\n",
    "        val_real_B = val_real_B.cuda()\n",
    "\n",
    "    val_fake_A = G_BA(val_real_B.detach())\n",
    "    val_fake_B = G_AB(val_real_A.detach())\n",
    "\n",
    "    stacked_image = torch.cat((val_real_A.data, val_fake_B.data, val_real_B.data, val_fake_A.data), 0)\n",
    "    save_image(stacked_image, 'generated_images/%s/%d_%d.png' % (dataset_name, epoch_count, batch_count), nrow=val_batch_size, normalize=True)\n",
    "#####################################\n",
    "\n",
    "\n",
    "################################################\n",
    "# Training loop\n",
    "################################################\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    for i, AB_img_batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get batch of images from A and B set\n",
    "        real_A = Variable(torch.FloatTensor(AB_img_batch['A']))\n",
    "        real_B = Variable(torch.FloatTensor(AB_img_batch['B']))\n",
    "\n",
    "        # Create matrix of real and fake target values for discriminator MSE loss\n",
    "        real = Variable(torch.FloatTensor(np.ones((real_A.size(0), *patch_size))), requires_grad=False)\n",
    "        fake = Variable(torch.FloatTensor(np.zeros((real_A.size(0), *patch_size))), requires_grad=False)\n",
    "\n",
    "        # Send variables to gpu\n",
    "        if GPU:\n",
    "            real_A = real_A.cuda()\n",
    "            real_B = real_B.cuda()\n",
    "            real = real.cuda()\n",
    "            fake = fake.cuda()\n",
    "\n",
    "        ##############################################\n",
    "        # Train the Generator Networks simultaneously\n",
    "        ##############################################\n",
    "\n",
    "        # zero out G Net gradients\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Adverserial (GAN) loss\n",
    "        ## Generate AB fakes, send through D nets and compute MSE loss\n",
    "        fake_A = G_BA(real_B)\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_GAN_AB = loss_GAN(D_B(fake_B), real)\n",
    "        loss_GAN_BA = loss_GAN(D_A(fake_A), real)\n",
    "        total_loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        ## Send generated fakes back through opposite G nets, compute L1 loss with original reals\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_A = loss_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = loss_cycle(recov_B, real_B)\n",
    "        total_loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Identity loss\n",
    "        ## Send real imgs through their own generator, compute L1 loss with original reals\n",
    "        loss_id_A = loss_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = loss_identity(G_AB(real_B), real_B)\n",
    "        total_loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # Total GAN loss (weighted sum of adverserial, cycle, and identity losses)\n",
    "        loss_G = total_loss_GAN + (lambda_cyc * total_loss_cycle) + (lambda_id * total_loss_identity)\n",
    "\n",
    "        # Backpropagate through the generator networks\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "        # Train the A Discriminator Network\n",
    "        ####################################\n",
    "\n",
    "        # zero out D_A gradients\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # compute loss using real and fake A imgs sent through D_A\n",
    "        loss_real = loss_GAN(D_A(real_A), real)\n",
    "        fake_A_ = get_buffer_batch(fake_A_buffer, fake_A, buffer_size=50)\n",
    "        loss_fake = loss_GAN(D_A(fake_A_.detach()), fake)\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # backpropagate D_A network\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        ####################################\n",
    "\n",
    "\n",
    "        ####################################\n",
    "        # Train the B Discriminator Network\n",
    "        ####################################\n",
    "\n",
    "        # zero out D_B gradients\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # compute loss using real and fake B imgs sent through D_B\n",
    "        loss_real = loss_GAN(D_B(real_B), real)\n",
    "        fake_B_ = get_buffer_batch(fake_B_buffer, fake_B, buffer_size=50)\n",
    "        loss_fake = loss_GAN(D_B(fake_B_.detach()), fake)\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # backpropagate D_B network\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        ##############################################\n",
    "\n",
    "\n",
    "        # Total Discriminator loss from both networks\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # Print the loss values every batch if verbose is enabled\n",
    "        if verbose:\n",
    "            description = \"\\r[D loss: {}] [G loss: {}, adverserial: {}, cycle: {}, identity: {}]\".format(loss_D.item(), loss_G.item(), total_loss_GAN.item(), total_loss_cycle.item(), total_loss_identity.item())\n",
    "            print(description)\n",
    "\n",
    "        # Periodicaly generate images from validation set\n",
    "        if i % sample_val_batch == 0:\n",
    "            generate_val_imgs(epoch, i)\n",
    "\n",
    "\n",
    "    # Update the learning rates using annealing scheduler\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # save model files every epoch\n",
    "    if epoch % checkpoint_epoch == 0:\n",
    "        torch.save(G_AB.state_dict(), 'checkpoints/%s/G_AB_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(G_BA.state_dict(), 'checkpoints/%s/G_BA_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), 'checkpoints/%s/D_A_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), 'checkpoints/%s/D_B_%d.pth' % (dataset_name, epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
